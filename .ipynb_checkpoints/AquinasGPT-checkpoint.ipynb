{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88dd479",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# AquinasGPT\n",
    "<br>\n",
    "\n",
    "The objective of this project is to lay the ground work for a chatbot that articulates its responses akin to the style found in Thomas Aquinas' Summa Theologiae. This project holds special importance to me, given my study of the Summa at \n",
    "[Thomas Aquinas College](https://www.thomasaquinas.edu/) (I even wrote my senior thesis on Aquinas' work the Division and Method of the Sciences which can be found [here](https://www.linkedin.com/in/jonscheaffer/overlay/1635484058296/single-media-viewer/?profileId=ACoAAC4PAjwBaKwgpkGvOJTXmVHGwbVUXyklzvk)). Additionally, the Summa's distinctive structure of question and response lends itself well to being adapted into the format of prompts and completions. Initially, I shall employ a prompt engineering approach, followed by a subsequent attempt at fine-tuning.\n",
    "\n",
    "<br>\n",
    "\n",
    "For the purposes of this project I will be testing my prompts with the questions:\n",
    "\n",
    "1.) What is the meaning of Life?\n",
    "\n",
    "2.) Is the pursuit of career advancement the ultimate source of meaning in life?\n",
    "\n",
    "3.) Is Data Science the greatest of all career paths?\n",
    "\n",
    "<br>\n",
    "\n",
    "I purposely start vague and then go more specific. My goal with this is to analyze the differences between Prompt Engineering and Fine-Tuning. My hypothesis is that Fine-tuning will work best for 1 and 2 but will struggle with 3. Further, Prompt Engineering may yield the best results for 3 given its flexibility, however it may fall short in style and format on 1 and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f34ef371",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What is the meaning of Life\"\n",
    "question2 = \"Is the pursuit of career advancement the ultimate source of meaning in life?\"\n",
    "question3 = \"Is the Data Science greatest of all career paths\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7090a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and API key \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ.get('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb2e97",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "<br>\n",
    "\n",
    "### First Attempt with Vague Instructions\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f35e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e00dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  f\"\"\" \n",
    "    In the same arguement format and style that was used by Thomas Aquinas in his Summa Theologiae\\\n",
    "    answer the question [{question1}]. Answer should be from perspectice of the teachings of the \\\n",
    "    catholic church (Though this does not have to be explicitly mentioned)\\\n",
    "    \"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640d27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  f\"\"\" \n",
    "    In the same arguement format and style that was used by Thomas Aquinas in his Summa Theologiae\\\n",
    "    answer the question [{question2}]. Answer should be from perspectice of the teachings of the \\\n",
    "    catholic church (Though this does not have to be explicitly mentioned)\\\n",
    "    \"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df9a3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  f\"\"\" \n",
    "    In the same arguement format and style that was used by Thomas Aquinas in his Summa Theologiae\\\n",
    "    answer the question [{question3}]. Answer should be from perspectice of the teachings of the \\\n",
    "    catholic church (Though this does not have to be explicitly mentioned)\\\n",
    "    \"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b568f3c",
   "metadata": {},
   "source": [
    "### Second Attempt with Specific Instructions\n",
    "<Br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\" Answer in a clear way, in the manner of Thomas Aquinas\n",
    "    not contradicting the teaching of the catholic church, the question [{question1}]\n",
    "    answers should be in the format:\n",
    "    \n",
    "    Obj. 1: ...\n",
    "    ...\n",
    "    Obj. N: ...\n",
    "    \n",
    "    On the contrary, ...(concise)\n",
    "    \n",
    "    I answer that, ...(more nuanced)\n",
    "    \n",
    "    Reply to Obj. 1: ... (Note: if the \"I answer that\" answers a objection fully then no need for that reply)\n",
    "    ...\n",
    "    Reply to Obj. N: ...\n",
    "    \"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\" Answer in a clear way, in the manner of Thomas Aquinas\n",
    "    not contradicting the teaching of the catholic church, the question [{question2}]\n",
    "    answers should be in the format:\n",
    "    \n",
    "    Obj. 1: ...\n",
    "    ...\n",
    "    Obj. N: ...\n",
    "    \n",
    "    On the contrary, ...(concise)\n",
    "    \n",
    "    I answer that, ...(more nuanced)\n",
    "    \n",
    "    Reply to Obj. 1: ... (Note: if the \"I answer that\" answers a objection fully then no need for that reply)\n",
    "    ...\n",
    "    Reply to Obj. N: ...\n",
    "    \"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd25ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\" Answer in a clear way, in the manner of Thomas Aquinas\n",
    "    not contradicting the teaching of the catholic church, the question [{question3}]\n",
    "    answers should be in the format:\n",
    "    \n",
    "    Obj. 1: ...\n",
    "    ...\n",
    "    Obj. N: ...\n",
    "    \n",
    "    On the contrary, ...(concise)\n",
    "    \n",
    "    I answer that, ...(more nuanced)\n",
    "    \n",
    "    Reply to Obj. 1: ... (Note: if the \"I answer that\" answers a objection fully then no need for that reply)\n",
    "    ...\n",
    "    Reply to Obj. N: ...\n",
    "    \"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27408f",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-Tuning\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf980685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading csv cleaned in Data_prep file\n",
    "ft = pd.read_csv(\"fine_tuning.csv\")[[\"prompt\", \"completion\"]]\n",
    "\n",
    "#Coverting to Json\n",
    "ft.to_json(\"AquinasGPT.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49ae3948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 582 prompt-completion pairs\n",
      "- There are 13 examples that are very long. These are rows: [194, 208, 245, 352, 358, 366, 367, 385, 421, 437, 530, 580, 581]\n",
      "For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n",
      "- All prompts end with suffix `\\n\\n###\\n\\n`\n",
      "- All completions end with suffix `\\nEND`\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove 13 long examples [Y/n]: Y\n",
      "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified file to `AquinasGPT_prepared (2).jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"AquinasGPT_prepared (2).jsonl\"\n",
      "\n",
      "After youâ€™ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"\\n    \\nEND\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 28.26 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f AquinasGPT.jsonl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c78e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6326f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262452f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.Completion.create(\n",
    "    model=FINE_TUNED_MODEL,\n",
    "    prompt=question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b639fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.Completion.create(\n",
    "    model=FINE_TUNED_MODEL,\n",
    "    prompt=question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aeedae",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.Completion.create(\n",
    "    model=FINE_TUNED_MODEL,\n",
    "    prompt=question3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
